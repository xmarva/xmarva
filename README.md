# üëã Hi, I‚Äôm Eva

I'm an AI Engineer and Researcher with a current focus on **Computer Vision** and **Generative AI**. \
Learn more about my approach to AI and work experience on my [personal website](https://xmarva.github.io). 

## üß† My Work Principles

- **Simplify the complex**  
  Aim to explain challenging concepts in plain language, avoiding the dense jargon often found in academic papers.  

- **Avoid unnecessary work**  
  Take the time to assess whether it's truly worth it and how to approach it in the most efficient way possible.  

- **Organize for success**  
  Time spent organizing and maintaining clean code and workflows pays off not just in the long term but immediately.  

- **Respect technological boundaries**  
  Strive to recognize the limits of technology and avoid playing the role of omnipotent creator.  

- **Stay humble**  
  A little less ego goes a long way in achieving better results.  


## ‚úçÔ∏è Latest Blogposts
<!-- BLOG-POST-LIST:START -->
- [Somatic Marker Hypothesis by Antonio Damasio](https://xmarva.github.io/blog/2025/antonio-damasio/)
- [LLM Inference Optimization](https://xmarva.github.io/blog/2025/inference-optimization/)
- [PEFT Method Overview [implementing Adapters in PyTorch]](https://xmarva.github.io/blog/2025/adapters/)
- [Physical Symbol Systems and the Language of Thought](https://xmarva.github.io/blog/2025/minds-as-computers/)
- [Building a Transformer &lpar;Cross-Attention and MHA Explained&rpar;](https://xmarva.github.io/blog/2025/building-a-transformer/)
<!-- BLOG-POST-LIST:END -->


## üõ†Ô∏è Technologies I Work With  

- `Python`, `C++`, `Wolfram`, `LaTex`, `Git`

- `NumPy`, `Pandas`, `Matplotlib`, `Plotly`

- `PyTorch`, `Lightning`, `Huggitng Face libs`, `OpenCV`
  
- `MlFlow`, `DVC`, `Weights & Biases`, `Hydra`, `Optuna`, `Prometheus`, `Grafana` 

- `Flask`, `FastAPI`, `Docker`, `CI/CD`, `AWS SageMaker`, `Gradio`, `Streamlit`, `vLLM`

## üîç Current Projects

- [Transformer Architectures Course](https://github.com/xmarva/transformer-architectures). \
  Deep exploration of transformer-based architectures such as BERT, GPT, T5, and others.
- [VisualTransformer: Adaptation and Fine-tuning with JAX/Flax](https://github.com/xmarva/jax-vit) \
  I'm working on adapting the Vision Transformer (ViT) model for computer vision tasks and optimizing it using JAX and Flax.
